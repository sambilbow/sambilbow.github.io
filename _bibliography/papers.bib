@inproceedings{bilbow2020,
  title = {Impact on Human Perception and Expression, Using Augmented Reality Technology as a Medium for Computational Art},
  booktitle = {Internal {{Research Proposal}}},
  author = {Bilbow, Sam},
  year = {2020},
  month = jan,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.7421529},
  urldate = {2022-12-10},
  abstract = {This research proposal develops multi-sensory AR technology as a novel method to address the current visual bias in AR development, through the question: What impact do multi-sensory AR experiences have on human perception and expression in participatory installation art? I outline contemporary literature that discusses the possibility for multi-sensory AR, as well as examples of installations that utilise non-visual modes of perceptual mediation. I will use an iterative design process to create my own multi- sensory AR instruments, which will be used in an installation. Through individual and group studies of the instrument and installation respectively, I aim to answer the question through a combination of rigorous survey-based quantitative data analysis, as well as a grounded theory approach to qualitatively analyse interview feedback. The findings would culminate in a thesis paper exploring the impact of augmented reality technology on user perception and expression, and the creation of a software tool that could be used as a research platform and framework for creating multi- sensory AR experiences that are immersive for both artist and audience.},
  langid = {english},
  keywords = {augmented reality; music technology; sonic art; installation; instrumentality},
  website = {https://dx.doi.org/10.5281/zenodo.7421529},
  dimensions = {false},
  preview = {2020-internal.png}
}

@inproceedings{bilbow2021,
  title = {The {{Value}} of {{Sound}} within a {{Multisensory Approach}} to {{AR}} in the {{Arts}}},
  booktitle = {Proceedings of the {{Multisensory Augmented Reality Workshop}}},
  author = {Bilbow, Sam and Kiefer, Chris and Chevalier, C{\'e}cile},
  year = {2021},
  pages = {8},
  publisher = {{Interact}},
  address = {{Italy}},
  doi = {10.5281/zenodo.7421488},
  abstract = {We explore the potential of sound within broader multisensory augmented reality, and its value in creating coherent, immersive and embodied experiences in computational art. We demonstrate this practically through accounts of the authors experiences in creating two pieces. Looking at the wider place of AR in the arts, we argue that DIY approaches to augmented reality are essential for creative work, and we speculate on how art can contribute to future theory, technologies and practice in the field.},
  langid = {english},
  website = {https://dx.doi.org/10.5281/zenodo.7421488},
  dimensions = {false},
  preview = {2021-mar.png}
}

@article{bilbow2021a,
  title = {The area\textasciitilde{} System: {{Exploring}} Real and Virtual Environments through Gestural Ambisonics and Audio Augmented Reality},
  author = {Bilbow, Sam},
  abstract = {In this paper, I outline the development and evaluation of the area~ system. area~ enables users to record, manipulate, and spatialise virtual audio samples or nodes around their immediate environment. Through a combination of ambisonics audio rendering and hand gesture tracking, this system calls attention to the ability of non-visual augmented reality (AR), here, audio augmented reality (AAR), to provide new aesthetic experiences of real and virtual environments. The system is contextualised within the move in computational art, and indeed, broader human computer interaction research, towards multisensory interaction. In particular, area~ is situated in the creative practice of works using multisensory AR as a medium to create expressive computational art. 

Through an autobiographical design study, these experiences are discussed in relation to the research question: “How can we better understand relationships between virtual and real environments through gesture and virtually placed audio augmented reality objects?” This hypothesis study proposes that new aesthetic experiences can result from the system and are waiting to be tested through user studies. The adoption of the Project North Star open-source AR head-mounted-display (HMD) could expand the possibilities of the area~ system by reducing the need to be tethered to a laptop and table for hand gesture input.

In discussing the future development of the system and my research, I propose a devising practice-led method for creating and evaluating new multisensory AR (MSAR) Experiences; as well as the tantalising prospect of adding interaction between other sensory modalities to the area~ system, such as vision or smell, which would be made possible by the use of this open-source HMD.},
  year = {2021},
  month = feb,
  journal = {Sonic Scope: New Approaches to Audiovisual Culture},
  volume = {2},
  doi = {10.21428/66f840a4.b74711a8},
  website = {https://dx.doi.org/10.21428/66f840a4.b74711a8},
  dimensions = {false},
  blog = {https://sambilbow.com/projects/area},
  code = {https://github.com/sambilbow/area},
  preview = {2021-sonicscope.png}
}

@inproceedings{bilbow2021b,
  title = {Developing {{Multisensory Augmented Reality As A Medium For Computational Artists}}},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  author = {Bilbow, Sam},
  year = {2021},
  month = feb,
  pages = {1--7},
  publisher = {{ACM}},
  address = {{Salzburg Austria}},
  doi = {10.1145/3430524.3443690},
  urldate = {2021-03-05},
  abstract = {This paper resituates multisensory augmented reality (MSAR) as an artistic medium for the creation of interactive and expressive works by computational artists. If an AR system can be thought of as one that combines real and virtual processes, is interactive in real-time, and is registered in three dimensions; why do we witness the majority of AR applications utilising primarily visual displays of information? In this paper, I propose a practice-led compositional approach for developing `MSAR Experiences', arguing that, as an medium that combines real and virtual multisensory processes, it must be explored with a multisensory approach. The paper further outlines the study methods that I will use to evaluate the developed experiences. The outcome of this project is the practice-led method as well as MSAR hardware, software and experiences that are developed and evaluated.},
  isbn = {978-1-4503-8213-7},
  langid = {english},
  website = {https://dx.doi.org/10.1145/3430524.3443690},
  dimensions = {false},
  preview = {2021-tei.png}
}

@inproceedings{bilbow2022,
  title = {Evaluating Polaris\textasciitilde{} - {{An Audiovisual Augmented Reality Experience Built}} on {{Open-Source Hardware}} and {{Software}}},
  booktitle = {{{NIME}} 2022},
  author = {Bilbow, Sam},
  year = {2022},
  month = jun,
  publisher = {{New Interfaces for Musical Expression PubPub}},
  address = {{The University of Auckland, New Zealand}},
  doi = {10.21428/92fbeb44.8abb9ce6},
  urldate = {2022-09-27},
  abstract = {Augmented reality (AR) is increasingly being envisaged as a process of perceptual mediation or modulation, not only as a system that combines aligned and interactive virtual objects with a real environment. Within artistic practice, this reconceptualisation has led to a medium that emphasises this multisensory integration of virtual processes, leading to expressive, narrative-driven, and thought-provoking AR experiences. This paper outlines the development and evaluation of the polaris\textasciitilde{} experience. polaris\textasciitilde{} is built using a set of open-source hardware and software components that can be used to create privacy-respecting and cost-effective audiovisual AR experiences. Its wearable component is comprised of the open-source Project North Star AR headset and a pair of bone conduction headphones, providing simultaneous real and virtual visual and auditory elements. These elements are spatially aligned using Unity and PureData to the real space that they appear in and can be gesturally interacted with in a way that fosters artistic and musical expression. In order to evaluate the polaris\textasciitilde, 10 participants were recruited, who spent approximately 30 minutes each in the AR scene and were interviewed about their experience. Using grounded theory, the author extracted coded remarks from the transcriptions of these studies, that were then sorted into the categories of Sentiment, Learning, Adoption, Expression, and Immersion. In evaluating polaris\textasciitilde{} it was found that the experience engaged participants fruitfully, with many noting their ability to express themselves audiovisually in creative ways. The experience and the framework the author used to create it is available in a Github respository.},
  langid = {english},
  website = {https://dx.doi.org/10.21428/92fbeb44.8abb9ce6},
  dimensions = {false},
  blog = {https://sambilbow.com/projects/polaris},
  code = {https://github.com/sambilbow/polaris},
  preview = {2022-nime.png}
}

@inproceedings{bilbow2023,
  title = {Mixed Realities as NIMEs (Upcoming)},
  booktitle = {{{NIME}} 2023 Workshop Proceedings},
  author = {Bilbow, Sam, and Wang, Yichen},
  year = {2023},
  month = jun,
  abstract = {The increased use of mixed reality (MR) as a platform for NIME development has appeared out of different disciplines, including the arts, humanities, and computer science, but are potentially restrained by their own fields. With this workshop, we aim to facilitate and co-construct a narrative around the role of MR as NIMEs, and develop a community built on existing concepts of MR through panel talks, demos and theory-generating discussions.},
  publisher = {{New Interfaces for Musical Expression}},
  address = {{The University of Auckland, New Zealand}},
  urldate = {2023-05-29},
  langid = {english},
  dimensions = {false},
  blog = {https://thexrt.space/nime-workshop},
  preview = {2023-nime.png}
}
