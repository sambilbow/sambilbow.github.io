<!doctype html>
<html prefix="og: http://ogp.me/ns#" lang="en">
<head>
    <title>Sam Bilbow | area~</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width" />
    <meta name="author" content="Sam Bilbow" />
    <meta name="copyright" content="Sam Bilbow" />
    <meta name="description" content="" />
    <meta property="og:title" content="Sam Bilbow | Title" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="https://sambilbow.com/past/path" />
    <meta property="og:image:height" content="450">
    <meta property="og:image:width" content="800">
    <meta property="og:description" content="" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sambilbow" />
    <meta name="twitter:creator" content="@sambilbow" />
    <meta name="twitter:title" content="Sam Bilbow | Title" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://sambilbow.com/past/path" />
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest.html">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5">
    <link rel="stylesheet" type="text/css" href="../../style.css" />
<style type="text/css">
    .embed-container {
        position: relative;
        padding-bottom: 56.25%;
        height: 0;
        overflow: hidden;
        max-width: 100%;
        margin: 0;
        background-color: white;
    }
    .embed-container iframe, .embed-container object, .embed-container embed {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        margin: 0;
    }
</style><script type="text/javascript">
var num_media = 3;
</script>
<script type="text/javascript" src="../../display_media.js"></script>

</head>
<body>

<div id="header">
    <div id="name"><a href="../../index.html" alt="about">sam bilbow</a></div>
    <div id="menu">
        <a href="../../projects/index.html" alt="projects" style="color: #d08770;">projects</a>
        <a href="../../writing/index.html" alt="writing">writing</a>
        <a href="../../engagements/index.html" alt="engagements">engagements</a>
    </div>
</div>
<div class="clear"></div>
<div class="clear"></div>


<main role="main">




<div id="content">

<div id="media">
    <div id="media_object">
        
            <div id="media_0" class="embed-container"><iframe onload="document.getElementById('media_object').style.opacity='1'" src="https://www.youtube.com/embed/SPd-f2EXuIQ" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div>
        
        
        
            
        
    </div>
    <p class="note"><b><i>this is a 360 / ambisonic video. that means that you can drag the video around in 3 dimensions and hear the sound pan around you. wear headphones for best results.</b></i></p>
</div>

<div class="clear"></div>
   
<div class="info">
    <span class="title">area~ (May, 2020)</span><br />
    
    
</div>

<div id="text" class="text">
    <h3><a href="../../writing/index.html">submission to sonicscope journal</a></h3>
    <h3>summary</h3>
    <p>
        The area~ system, which stands loosely for ‘augmented reality environmental audio’ aims to afford users the ability to spectromorphologically (defined by Smalley to concern spatial, temporal and textural qualities of sound <a href="https://www.cambridge.org/core/journals/organised-sound/article/spectromorphology-explaining-soundshapes/A18EBE591592836FC22C20FB327D3232">(1997)</a>) manipulate sounds from their environment into a virtual audio environment. Through bone conduction headphones and head tracking, this sound field is heard in synchronicity with their actual environment. The system was created in order to explore and reveal the relationship between real and virtual environments.
    </p>
    <p>
    
        Through an autobiographical design study, these experiences are discussed in relation to the research question: “How can we better understand relationships between virtual and real environments through gesture and virtually placed audio augmented reality objects?” This hypothesis study proposes that new aesthetic experiences can result from the system and are waiting to be tested through user studies. The adoption of the <a href=../2020_12msar/index.html>North Star open-source AR head-mounted-display</a> could expand the possibilities of the area~ system by reducing the need to be tethered to a laptop and table for hand gesture input.
    </p>

    <p>
        Overall, the results from ABD have shown that area~ is an effective tool for examining the combinatorial relationship between real and virtual environments. Despite the systems hardware setup needing to be worked on to allow for quicker start up and more accurate head tracking, it has provided me with novel aesthetic experience through:
        <ul>
            <li>
                The blending of real and virtual auditory environments to create a third, augmented environment that was greater in experiential nature than the sum of its parts (not simply a combinatorial layering)
            </li>
            <li>
                The ability to spectromorphologically manipulate sounds in real-time in this third environment with the body
            </li>
            <li>
                The potential for creating believable illusions of real-world sound sources from these manipulated and spatialised virtual sounds.
            </li>
        </ul>
    </p>

 

    <h3>technologies</h3>

   
    <p>
        The three technologies used in area~, are gestural hand tracking, rotational head tracking, and ambisonics. The gestural hand tracker used in the system is a Leap Motion LM-010 Controller, a USB infrared camera device that provides location and orientation data output of individual finger joints (and therefore hands) when they are presented above the device. The Leap Motion Controller (LMC) has been adopted in a multitude of settings such as being mounted on VR headsets (Leap Motion 2016), and converting hand gestures to MIDI (Leap Motion 2017). More recently, UltraLeap are investigating the use of this same technology with gesture-based public information screens to help combat the “hygiene risks of touch screens” (2020).
    </p>
    
    <p>
        Rotational head tracking is achieved via an inertial measurement unit (IMU). This small and inexpensive component provides orientational data output at 20 times a second. When affixed to the head via a headset or headphones, it is a relatively easy and cheap way of implementing head tracking into the system.
    </p>
    
    <p>
        Ambisonics is an audio format that allows for full-spherical audio capture and playback (Gerzon 1973). There are four recorded channels (referred to as A-Format) that unlike regular mono, stereo, or surround sound formats, contain no information about the speakers it is delivering the signal to. Rather, these channels can be encoded in such a way as to describe a three-dimensional field of sound referred to as B-Format. B-Format can be decoded through “virtual microphones”, any number of which can be placed within this three-dimensional sound field to provide standard channel outputs. 
    </p>
    
    <p>
        For example, in area~, I have used a RØDE Soundfield NTSF-1 microphone array comprised of 4 microphones. The A-Format output is encoded to B-Format by an audio plugin. A software library I have configured, further decodes the B-Format to two responsive, binaural, virtual audio output channels. This all occurs in real-time; and as the user moves their head, the microphones inside the three-dimensional sound field rotate proportionally, providing realistic changes to what is heard.
    </p>

    <h3>experience</h3>

    <p>
        The recording or ‘sampling’ stage is initiated by making a left-hand grab above the LMC, the longer lasting the grab, the longer the portion of audio from the ambisonic palette is sampled. The three-dimensional coordinates of the hand above the LMC correlates with the location of audio recorded (this is achieved by mapping the hand coordinates to a virtual microphone inside the ambisonic palette), essentially allowing the user to record sounds around their person in three dimensions. Upon letting go of the grab gesture, the sample plays on repeat (using the karma~ Library (Constanzo 2015)) through the bone conduction headphones, thus setting up the session’s virtual audio environment.
    </p>

    <p>
        The manipulation stage is automatically initiated after the ending of the previous grab gesture and uses translational (x, y, z) and rotational (roll, pitch) values from both hands when above the LMC. There are two audio effects being manipulated, with parameters from these effects mapped in different ways to the translation and rotation of the user’s hands. When the user decides to end manipulating the sample, they can do so by performing a grab with both hands. Once this happens, the band-pass filter and granular synthesis parameters are frozen for that sample.
    </p>
    <p>
        The spatialise stage begins once the manipulation stage is ended by the user. The three-dimensional space above the LMC is mapped to the virtual audio environment, in which the user is currently listening to the sample that they have recorded. The user can use their right hand to move the sample around the virtual audio environment. For an example of the effect this has, moving the hand between the two extremes of the x-axis (left to right) results in hearing the sample move from ear to ear. The spatialise stage is ended by grabbing with the right hand.
    </p>

    <p>
        Once the spatialise stage has ended, the user has the option to repeat the process 7 more times, allowing for the creation of a virtual audio environment comprised of up to 8 spatialised audio samples, or what I refer to as nodes. Below are some examples of these virtual audio environments.
    </p>
    <div id="audio">
        <iframe frameborder="0" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/959050348&amp;color=d08770&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;inverse=true&amp;font=Helvetica"></iframe>
    </div>
    <div id="audio">
        <iframe frameborder="0" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/959050342&amp;color=d08770&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;inverse=true&amp;font=Helvetica"></iframe>
    </div>
    <div id="audio">
        <iframe frameborder="0" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/959050333&amp;color=d08770&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;inverse=true&amp;font=Helvetica"></iframe>
    </div>
    <div id="audio">
        <iframe frameborder="0" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/959050357&amp;color=d08770&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;inverse=true&amp;font=Helvetica"></iframe>
    </div>
</div>




</div>

</main>

<script type="text/javascript">
</script>
</body>
</html>