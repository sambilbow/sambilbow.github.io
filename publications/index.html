<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Sam  Bilbow, PhD</title>    
    <meta name="author" content="Sam  Bilbow, PhD">
    <meta name="description" content="Sam Bilbow - XR and Audio Software Developer based in Norwich.
">
    <meta name="keywords" content="academic-website, portfolio-website, augmented-reality, music-technology">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link href="https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&amp;display=swap" rel="stylesheet"> 
    <link href="https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&amp;display=swap" rel="stylesheet">
    

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sambilbow.com/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Sam Bilbow, PhD<span> </span><img class="rounded-circle" id="emoji" src="/assets/img/emoji/catsty.gif"></a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/collaborations/">collaborations</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/workshops/">workshops</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2024</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2024-thesis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2024-thesis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2024" class="col-sm-8">
        <!-- Title -->
        <div class="title">Material, Embodied, and Spatial Relations in Augmented Reality: An Exploration of AR as a Medium for Musical Composition and Performance</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Doctoral Thesis, University of Sussex</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="./" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>It has been thirty years since the original definition of augmented reality (AR) as a technology used to ‘augment the visual field of a user with information necessary in the performance of tasks’. In this first instance, it was developed with the purpose to ‘improve the efficiency and quality of human workers in their performance of manufacturing activities’ (Caudell and Mizell, 1992). Alongside subsequent decades of funding from the U.S. military-industrial complex (MIC), we have also seen the uptake and reappropriation of AR in creative fields, such as computational art, performance, design, and entertainment - these works often proposing do-it-yourself (DIY) and open-source approaches to their design.
    
    Despite these developments, AR within sound-driven forms of art have been relatively under-explored. If an AR system can be thought of as one that can combine real and virtual multisensory processes, is interactive in real-time, and is registered in three dimensions (Azuma, 1997); why do we, thirty years on, witness the paradigmatic form of AR still being heavily biased (Billinghurst et al., 2015) towards it being a method of visual information overlay?

    Standing in stark contrast to the currently unfolding and hyper-commercialised view of AR – as defined by the corporate ‘Metaverse’ – this thesis resituates AR as an artistic medium for the creation of interactive and expressive works by musicians and sound artists. It is guided primarily by the questions: ‘What are AR’s affordances as an artistic medium, what is the resultant experience for participants and audiences (or ‘immersants’) in these experiences, and what might a future of AR digital music instruments look, sound, and feel like?’

    To address these questions, this practice-based research takes a DIY Approach to Sound ARt, arguing that, as an medium that combines real and virtual multisensory processes, it must explored with a sensory-process agnostic approach – that is, to approach AR as more than mere visual-information overlay, instead as ‘real-time computationally mediated perception’ (Chevalier and Kiefer, 2020). This has involved making and hacking technology as a necessary aesthetic and political stance against commercial AR technologies in their typical form.

    Three sound augmented reality art (ARt) experiences are outlined, and embody the majority of the practical contribution of this thesis: area , polaris , and polygons . In discussing the results of these three study chapters, theoretical propositions are made: ‘augmented materiality’, ‘augmented embodiment’, and ‘augmented space’, that have implications for the use of AR as a sonic medium. Moreover, out of the iterated design of the AR experiences, their study, evaluation, and discussion, three ‘design guidelines’ for those in the field interested in reproducing or devel- oping similar sound ARt have been developed: Designing for Rich AR Experience, Consideration of the AR Instrument, and Role of the Virtual in the AR Environment.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2023-nime.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2023-nime.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2023" class="col-sm-8">
        <!-- Title -->
        <div class="title">Mixed Realities as NIMEs</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>, and <a href="https://yichenwangs.github.io/" rel="external nofollow noopener" target="_blank">Yichen Wang</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In NIME 2023 Workshop Proceedings</em>, Jun 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.5281/zenodo.8026943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The increased use of mixed reality (MR) as a platform for NIME development has appeared out of different disciplines, including the arts, humanities, and computer science, but are potentially restrained by their own fields. With this workshop, we aim to facilitate and co-construct a narrative around the role of MR as NIMEs, and develop a community built on existing concepts of MR through panel talks, demos and theory-generating discussions.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2022-nime.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2022-nime.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2022" class="col-sm-8">
        <!-- Title -->
        <div class="title">Evaluating Polaris~ - An Audiovisual Augmented Reality Experience Built on Open-Source Hardware and Software</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In NIME 2022</em>, Jun 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://sambilbow.com/projects/polaris" class="btn btn-sm z-depth-0" role="button">Blog</a>
            <a href="https://github.com/sambilbow/polaris" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://dx.doi.org/10.21428/92fbeb44.8abb9ce6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Augmented reality (AR) is increasingly being envisaged as a process of perceptual mediation or modulation, not only as a system that combines aligned and interactive virtual objects with a real environment. Within artistic practice, this reconceptualisation has led to a medium that emphasises this multisensory integration of virtual processes, leading to expressive, narrative-driven, and thought-provoking AR experiences. This paper outlines the development and evaluation of the polaris~ experience. polaris~ is built using a set of open-source hardware and software components that can be used to create privacy-respecting and cost-effective audiovisual AR experiences. Its wearable component is comprised of the open-source Project North Star AR headset and a pair of bone conduction headphones, providing simultaneous real and virtual visual and auditory elements. These elements are spatially aligned using Unity and PureData to the real space that they appear in and can be gesturally interacted with in a way that fosters artistic and musical expression. In order to evaluate the polaris~, 10 participants were recruited, who spent approximately 30 minutes each in the AR scene and were interviewed about their experience. Using grounded theory, the author extracted coded remarks from the transcriptions of these studies, that were then sorted into the categories of Sentiment, Learning, Adoption, Expression, and Immersion. In evaluating polaris~ it was found that the experience engaged participants fruitfully, with many noting their ability to express themselves audiovisually in creative ways. The experience and the framework the author used to create it is available in a Github respository.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2021-mar.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2021-mar.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2021" class="col-sm-8">
        <!-- Title -->
        <div class="title">The Value of Sound within a Multisensory Approach to AR in the Arts</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>, <a href="https://profiles.sussex.ac.uk/p208667-chris-kiefer" rel="external nofollow noopener" target="_blank">Chris Kiefer</a>, and <a href="https://profiles.sussex.ac.uk/p235751-cecile-chevalier" rel="external nofollow noopener" target="_blank">Cécile Chevalier</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the Multisensory Augmented Reality Workshop</em>, Sep 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dx.doi.org/10.5281/zenodo.7421488" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We explore the potential of sound within broader multisensory augmented reality, and its value in creating coherent, immersive and embodied experiences in computational art. We demonstrate this practically through accounts of the authors experiences in creating two pieces. Looking at the wider place of AR in the arts, we argue that DIY approaches to augmented reality are essential for creative work, and we speculate on how art can contribute to future theory, technologies and practice in the field.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2021-tei.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2021-tei.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2021b" class="col-sm-8">
        <!-- Title -->
        <div class="title">Developing Multisensory Augmented Reality As A Medium For Computational Artists</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dx.doi.org/10.1145/3430524.3443690" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper resituates multisensory augmented reality (MSAR) as an artistic medium for the creation of interactive and expressive works by computational artists. If an AR system can be thought of as one that combines real and virtual processes, is interactive in real-time, and is registered in three dimensions; why do we witness the majority of AR applications utilising primarily visual displays of information? In this paper, I propose a practice-led compositional approach for developing ‘MSAR Experiences’, arguing that, as an medium that combines real and virtual multisensory processes, it must be explored with a multisensory approach. The paper further outlines the study methods that I will use to evaluate the developed experiences. The outcome of this project is the practice-led method as well as MSAR hardware, software and experiences that are developed and evaluated.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2021-sonicscope.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2021-sonicscope.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2021a" class="col-sm-8">
        <!-- Title -->
        <div class="title">The area~ System: Exploring Real and Virtual Environments through Gestural Ambisonics and Audio Augmented Reality</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Sonic Scope: New Approaches to Audiovisual Culture</em>, Feb 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://sambilbow.com/projects/area" class="btn btn-sm z-depth-0" role="button">Blog</a>
            <a href="https://github.com/sambilbow/area" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://dx.doi.org/10.21428/66f840a4.b74711a8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, I outline the development and evaluation of the area  system. area  enables users to record, manipulate, and spatialise virtual audio samples or nodes around their immediate environment. Through a combination of ambisonics audio rendering and hand gesture tracking, this system calls attention to the ability of non-visual augmented reality (AR), here, audio augmented reality (AAR), to provide new aesthetic experiences of real and virtual environments. The system is contextualised within the move in computational art, and indeed, broader human computer interaction research, towards multisensory interaction. In particular, area  is situated in the creative practice of works using multisensory AR as a medium to create expressive computational art. 

Through an autobiographical design study, these experiences are discussed in relation to the research question: “How can we better understand relationships between virtual and real environments through gesture and virtually placed audio augmented reality objects?” This hypothesis study proposes that new aesthetic experiences can result from the system and are waiting to be tested through user studies. The adoption of the Project North Star open-source AR head-mounted-display (HMD) could expand the possibilities of the area  system by reducing the need to be tethered to a laptop and table for hand gesture input.

In discussing the future development of the system and my research, I propose a devising practice-led method for creating and evaluating new multisensory AR (MSAR) Experiences; as well as the tantalising prospect of adding interaction between other sensory modalities to the area  system, such as vision or smell, which would be made possible by the use of this open-source HMD.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview">
<figure>
  
    <picture>
      
      <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="-480.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="-800.webp"></source>
      <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="-1400.webp"></source>
      

      <!-- Fallback to the original file -->
      <img src="/assets/img/publication_preview/2020-internal.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2020-internal.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

    </picture>
  
</figure>
</div>

        <!-- Entry bib key -->
        <div id="bilbow2020" class="col-sm-8">
        <!-- Title -->
        <div class="title">Impact on Human Perception and Expression, Using Augmented Reality Technology as a Medium for Computational Art</div>
        <!-- Author -->
        <div class="author">
        

        <em>Sam Bilbow</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Internal Research Proposal</em>, Jan 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dx.doi.org/10.5281/zenodo.7421529" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This research proposal develops multi-sensory AR technology as a novel method to address the current visual bias in AR development, through the question: What impact do multi-sensory AR experiences have on human perception and expression in participatory installation art? I outline contemporary literature that discusses the possibility for multi-sensory AR, as well as examples of installations that utilise non-visual modes of perceptual mediation. I will use an iterative design process to create my own multi- sensory AR instruments, which will be used in an installation. Through individual and group studies of the instrument and installation respectively, I aim to answer the question through a combination of rigorous survey-based quantitative data analysis, as well as a grounded theory approach to qualitatively analyse interview feedback. The findings would culminate in a thesis paper exploring the impact of augmented reality technology on user perception and expression, and the creation of a software tool that could be used as a research platform and framework for creating multi- sensory AR experiences that are immersive for both artist and audience.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2024 Sam  Bilbow, PhD. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>, and hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: February 14, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
